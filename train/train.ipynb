{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import  CustomDataset\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "import random\n",
    "# import rasterio\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.utils import shuffle as shuffle_lists\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "import torch\n",
    "\n",
    "#default\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#trasnform\n",
    "from torchvision import transforms\n",
    "\n",
    "#dataset\n",
    "from utils.dataset import CustomDataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torchvision.models as models\n",
    "from torchsummary import summary\n",
    "\n",
    "from utils import models \n",
    "#metric\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import torchmetrics.functional as tf\n",
    "\n",
    "#numeric\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#system \n",
    "from tqdm import tqdm\n",
    "import os \n",
    "import wandb\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################## SAVE TIME ##############################\n",
      "############################## 202403140852 ##############################\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "weekday = now.strftime(\"%A\")  # %A는 현재 요일을 나타내는 포맷 지정자입니다.\n",
    "formatted_now = now.strftime(\"%Y%m%d%H%M\")\n",
    "checkpoint_datetime = formatted_now\n",
    "print(f\"#\"*30, \"SAVE TIME\", \"#\"*30)\n",
    "print(f\"#\"*30, checkpoint_datetime, \"#\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = '../../dataset/train_meta.csv'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    # transforms.Resize((512,512), antialias= True),\n",
    "    # transforms.RandomApply([\n",
    "    #     transforms.RandomRotation(degrees = 30),\n",
    "    #     transforms.RandomVerticalFlip(p= 0.5),\n",
    "    #     transforms.RandomVerticalFlip(p=0.5),\n",
    "    #     transforms.RandomCrop(size =224, scale= (0.8, 1.2), ratio = (0.75, 1.2), antialias = True),\n",
    "    #     transforms.ColorJitter(brightness= 0.2, contrast = 0.2)\n",
    "    # ]),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "tr_batch = 2\n",
    "vl_batch = 2\n",
    "\n",
    "dataset = CustomDataset(\n",
    "    csv_path= train_csv,\n",
    "    transform= None, #None으로 세팅\n",
    "    MAX_PIXEL_VALUE= 65535,\n",
    "    band = (7,6,2) #기존 세팅 \n",
    ")\n",
    "\n",
    "# 훈련 및 검증 세트 분할\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size \n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = tr_batch, shuffle= True)\n",
    "valid_loader = DataLoader(dataset = valid_dataset, batch_size = vl_batch, shuffle= False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RasterioIOError",
     "evalue": "'../../dataset/train_img/train_img_10872.tif' not recognized as a supported file format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32mrasterio/_base.pyx:310\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_base.pyx:221\u001b[0m, in \u001b[0;36mrasterio._base.open_dataset\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mrasterio/_err.pyx:221\u001b[0m, in \u001b[0;36mrasterio._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: '../../dataset/train_img/train_img_10872.tif' not recognized as a supported file format.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRasterioIOError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, masks \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(images\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m     idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/fire/lib/python3.8/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/fire/lib/python3.8/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/fire/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/fire/lib/python3.8/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/anaconda3/envs/fire/lib/python3.8/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/spark/train/utils/dataset.py:44\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     41\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_paths[idx]\n\u001b[1;32m     42\u001b[0m mask_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask_paths[idx]\n\u001b[0;32m---> 44\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_img_arr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_mask_arr(path \u001b[38;5;241m=\u001b[39m mask_path)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# # transform \u001b[39;00m\n",
      "File \u001b[0;32m~/spark/train/utils/dataset.py:27\u001b[0m, in \u001b[0;36mCustomDataset.get_img_arr\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_img_arr\u001b[39m(\u001b[38;5;28mself\u001b[39m, path):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# img = rasterio.open(path).read().transpose((1, 2, 0))\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mrasterio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mband)\u001b[38;5;241m.\u001b[39mtranspose((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m     28\u001b[0m     img \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32(img)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMAX_PIXEL_VALUE\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/fire/lib/python3.8/site-packages/rasterio/env.py:451\u001b[0m, in \u001b[0;36mensure_env_with_credentials.<locals>.wrapper\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    448\u001b[0m     session \u001b[38;5;241m=\u001b[39m DummySession()\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m env_ctor(session\u001b[38;5;241m=\u001b[39msession):\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/fire/lib/python3.8/site-packages/rasterio/__init__.py:304\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, width, height, count, crs, transform, dtype, nodata, sharing, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m path \u001b[38;5;241m=\u001b[39m _parse_path(raw_dataset_path)\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 304\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mDatasetReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdriver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msharing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharing\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    306\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m get_writer_for_path(path, driver\u001b[38;5;241m=\u001b[39mdriver)(\n\u001b[1;32m    307\u001b[0m         path, mode, driver\u001b[38;5;241m=\u001b[39mdriver, sharing\u001b[38;5;241m=\u001b[39msharing, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    308\u001b[0m     )\n",
      "File \u001b[0;32mrasterio/_base.pyx:312\u001b[0m, in \u001b[0;36mrasterio._base.DatasetBase.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRasterioIOError\u001b[0m: '../../dataset/train_img/train_img_10872.tif' not recognized as a supported file format."
     ]
    }
   ],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(images.shape)\n",
    "    idx = 0\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images[idx])\n",
    "    plt.title('image')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(masks[idx], cmap= 'gray')\n",
    "    plt.title('mask')\n",
    "    plt.show()\n",
    "    break\n",
    "\n",
    "for images, masks in valid_loader:\n",
    "    print(images.shape)\n",
    "    idx = 0\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(images[idx])\n",
    "    plt.title('image')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(masks[idx], cmap= 'gray')\n",
    "    plt.title('mask')\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/eiden/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             864\n",
      "       BatchNorm2d-2         [-1, 32, 224, 224]              64\n",
      "              ReLU-3         [-1, 32, 224, 224]               0\n",
      "            Conv2d-4         [-1, 32, 224, 224]           9,216\n",
      "       BatchNorm2d-5         [-1, 32, 224, 224]              64\n",
      "              ReLU-6         [-1, 32, 224, 224]               0\n",
      "         MaxPool2d-7         [-1, 32, 112, 112]               0\n",
      "            Conv2d-8         [-1, 64, 112, 112]          18,432\n",
      "       BatchNorm2d-9         [-1, 64, 112, 112]             128\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "           Conv2d-11         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-12         [-1, 64, 112, 112]             128\n",
      "             ReLU-13         [-1, 64, 112, 112]               0\n",
      "        MaxPool2d-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15          [-1, 128, 56, 56]          73,728\n",
      "      BatchNorm2d-16          [-1, 128, 56, 56]             256\n",
      "             ReLU-17          [-1, 128, 56, 56]               0\n",
      "           Conv2d-18          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-19          [-1, 128, 56, 56]             256\n",
      "             ReLU-20          [-1, 128, 56, 56]               0\n",
      "        MaxPool2d-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 256, 28, 28]         294,912\n",
      "      BatchNorm2d-23          [-1, 256, 28, 28]             512\n",
      "             ReLU-24          [-1, 256, 28, 28]               0\n",
      "           Conv2d-25          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-26          [-1, 256, 28, 28]             512\n",
      "             ReLU-27          [-1, 256, 28, 28]               0\n",
      "        MaxPool2d-28          [-1, 256, 14, 14]               0\n",
      "           Conv2d-29          [-1, 512, 14, 14]       1,179,648\n",
      "      BatchNorm2d-30          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-31          [-1, 512, 14, 14]               0\n",
      "           Conv2d-32          [-1, 512, 14, 14]       2,359,296\n",
      "      BatchNorm2d-33          [-1, 512, 14, 14]           1,024\n",
      "             ReLU-34          [-1, 512, 14, 14]               0\n",
      "  ConvTranspose2d-35          [-1, 256, 28, 28]         524,544\n",
      "           Conv2d-36          [-1, 256, 28, 28]       1,179,648\n",
      "      BatchNorm2d-37          [-1, 256, 28, 28]             512\n",
      "             ReLU-38          [-1, 256, 28, 28]               0\n",
      "           Conv2d-39          [-1, 256, 28, 28]         589,824\n",
      "      BatchNorm2d-40          [-1, 256, 28, 28]             512\n",
      "             ReLU-41          [-1, 256, 28, 28]               0\n",
      "  ConvTranspose2d-42          [-1, 128, 56, 56]         131,200\n",
      "           Conv2d-43          [-1, 128, 56, 56]         294,912\n",
      "      BatchNorm2d-44          [-1, 128, 56, 56]             256\n",
      "             ReLU-45          [-1, 128, 56, 56]               0\n",
      "           Conv2d-46          [-1, 128, 56, 56]         147,456\n",
      "      BatchNorm2d-47          [-1, 128, 56, 56]             256\n",
      "             ReLU-48          [-1, 128, 56, 56]               0\n",
      "  ConvTranspose2d-49         [-1, 64, 112, 112]          32,832\n",
      "           Conv2d-50         [-1, 64, 112, 112]          73,728\n",
      "      BatchNorm2d-51         [-1, 64, 112, 112]             128\n",
      "             ReLU-52         [-1, 64, 112, 112]               0\n",
      "           Conv2d-53         [-1, 64, 112, 112]          36,864\n",
      "      BatchNorm2d-54         [-1, 64, 112, 112]             128\n",
      "             ReLU-55         [-1, 64, 112, 112]               0\n",
      "  ConvTranspose2d-56         [-1, 32, 224, 224]           8,224\n",
      "           Conv2d-57         [-1, 32, 224, 224]          18,432\n",
      "      BatchNorm2d-58         [-1, 32, 224, 224]              64\n",
      "             ReLU-59         [-1, 32, 224, 224]               0\n",
      "           Conv2d-60         [-1, 32, 224, 224]           9,216\n",
      "      BatchNorm2d-61         [-1, 32, 224, 224]              64\n",
      "             ReLU-62         [-1, 32, 224, 224]               0\n",
      "           Conv2d-63          [-1, 1, 224, 224]              33\n",
      "================================================================\n",
      "Total params: 7,763,041\n",
      "Trainable params: 7,763,041\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 309.31\n",
      "Params size (MB): 29.61\n",
      "Estimated Total Size (MB): 339.50\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# cost func & optimizer 정의\n",
    "\n",
    "# random seed 고정\n",
    "random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "else:\n",
    "    device = torch.device('cuda')\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "\n",
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=True)\n",
    "\n",
    "lr = 0.0001\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = lr)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "\n",
    "print(f'device : {device}')\n",
    "summary(model, input_size=(3, 224, 224))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    'tr_bce' : [],\n",
    "    'vl_bce' : []\n",
    "}\n",
    "epoch, epochs = 0, 200\n",
    "early_stopping_epochs, early_stopping_cnt= 10, 0\n",
    "best_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/200 [00:00<?, ?it/s]\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, (images, masks) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_loader)):\n\u001b[0;32m----> 6\u001b[0m     images, masks \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m, masks\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m      9\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m loss(preds, masks)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/eiden/lib/python3.8/site-packages/torch/cuda/__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m     )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(epoch, epochs)):\n",
    "    train_losses, valid_losses = 0., 0.\n",
    "    \n",
    "    model.train()\n",
    "    for _, (images, masks) in tqdm(enumerate(train_loader)):\n",
    "        images, masks = images.to(device), masks.to(device)\n",
    "        preds = model(images)\n",
    "        \n",
    "        train_loss = loss(preds, masks).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_losses += train_loss.item()\n",
    "        \n",
    "        \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for _, (images, masks) in tqdm(enumerate(train_loader)):\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            preds = model(images)\n",
    "            \n",
    "            valid_loss = loss(preds, masks).to(device)\n",
    "            valid_losses += valid_loss.item()\n",
    "            \n",
    "    metrics['tr_bce'].append(train_losses / len(train_loader))\n",
    "    metrics['vl_bce'].append(valid_losses / len(valid_loader))\n",
    "    \n",
    "    # Early Stopping\n",
    "    if valid_losses < best_loss:\n",
    "        best_loss = valid_losses\n",
    "        early_stopping_cnt = 0\n",
    "    else:\n",
    "        early_stopping_cnt += 1\n",
    "        if early_stopping_cnt >= early_stopping_epochs:\n",
    "            print(f\"Early Stops!!! : {epoch}/{epochs}\")\n",
    "    \n",
    "    # Validation 성능이 이전보다 우수한 경우 모델 저장\n",
    "    if valid_losses > np.array(metrics['vl_bce'])[:-1].max():\n",
    "        SAVE_DIR = '../result'\n",
    "        torch.save({\n",
    "            \"model\" : f\"{epoch}\",\n",
    "            \"epoch\" : epoch,\n",
    "            \"epochs\" : epochs,\n",
    "            \"model_state_dict\" : model.state_dict(),\n",
    "            \"optimizer_state_dict\" : optimizer.state_dict(),\n",
    "            \"learning_rate\" : lr,\n",
    "            \"loss\" : loss,\n",
    "            \"metric\" : metrics,\n",
    "            \"description\" : f\"segmentation model training status : {epoch}/{epochs}\"\n",
    "        },\n",
    "        f'../model/{checkpoint_datetime}.pt')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
